cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
folds = createFolds(y = imputed_dataset$High_Worth,
k = 10)
folds = createFolds(y = imputed_dataset$High_Worth,
k = 10)
accuracy = lapply(folds, function(x){
training_fold = imputed_dataset[-x,]
test_fold = imputed_dataset[x,]
folds_decisiontree_classifier = rpart(formula = High_Worth ~ .,
data = training_fold)
folds_y_pred = predict(decisiontree_classifier,
newdata = test_fold[,-28],
type = 'class')
folds_cm = table(test_fold[,28], folds_y_pred)
folds_accuracy = (folds_cm[1,1] + folds_cm[2,2])/(folds_cm[1,1] + folds_cm[1,2] + folds_cm[2,1] + folds_cm[2,2])
return(folds_accuracy)
})
mean_accuracy = mean(as.numeric(accuracy))
mean_accuracy
#load randomFprest package to build decision trees
library(randomForest)
#build the decision tree classifier using training set
randomforest_classifier = randomForest(formula = High_Worth ~ .,
data = training_set,
ntree = 100,
mtry = 5,
nodesize = 20)
y_pred = predict(randomforest_classifier,
newdata = test_set[,-28])
y_pred
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
folds = createFolds(y = imputed_dataset$High_Worth,
k = 10)
accuracy = lapply(folds, function(x){
training_fold = imputed_dataset[-x,]
test_fold = imputed_dataset[x,]
folds_randomforest_classifier = randomForest(formula = High_Worth ~ .,
data = training_fold,
ntree = 100,
mtry = 5,
nodesize = 20)
folds_y_pred = predict(randomforest_classifier,
newdata = test_fold[,-28],
type = 'class')
folds_cm = table(test_fold[,28], folds_y_pred)
folds_accuracy = (folds_cm[1,1] + folds_cm[2,2])/(folds_cm[1,1] + folds_cm[1,2] + folds_cm[2,1] + folds_cm[2,2])
return(folds_accuracy)
})
mean_accuracy = mean(as.numeric(accuracy))
mean_accuracy
library(gbm)
boostedtree_classifier = gbm(formula = High_Worth ~ .,
data = training_set,
n.trees = 100,
interaction.depth = 3)
#build the decision tree classifier using training set
boostedtree_classifier = gbm(formula = High_Worth ~ .,
data = training_set,
distribution = "bernoulli",
n.trees = 100,
interaction.depth = 3)
y_pred = predict(boostedtree_classifier,
newdata = test_set[,-28])
y_pred = predict(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 100)
y_pred
imputed_dataset$High_Worth = as.numeric(imputed_dataset$High_Worth)
#create the training and test set
split = sample.split(Y = imputed_dataset$High_Worth,
SplitRatio = 0.75)
training_set = subset(imputed_dataset, split == TRUE)
test_set = subset(imputed_dataset, split == FALSE)
#build the decision tree classifier using training set
boostedtree_classifier = gbm(formula = High_Worth ~ .,
data = training_set,
distribution = "bernoulli",
n.trees = 100,
interaction.depth = 3)
#predict the test set
y_pred = predict(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 100)
y_pred
imputed_dataset$High_Worth
imputed_dataset$High_Worth = as.vector(imputed_dataset$High_Worth)
imputed_dataset$High_Worth
class(imputed_dataset$High_Worth)
#create the training and test set
split = sample.split(Y = imputed_dataset$High_Worth,
SplitRatio = 0.75)
training_set = subset(imputed_dataset, split == TRUE)
test_set = subset(imputed_dataset, split == FALSE)
#build the decision tree classifier using training set
boostedtree_classifier = gbm(formula = High_Worth ~ .,
data = training_set,
distribution = "bernoulli",
n.trees = 100,
interaction.depth = 3)
#predict the test set
y_pred = predict(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 100)
#boosted tree model for oakhurst high worth prediction
#load caTools package to split the dataset into test and validation set
library(caTools)
#load mice package to impute the missing values
library(mice)
#load gbm package to build decision trees
library(gbm)
#load caret package to perform k-fold cross validation
library(caret)
#load data
setwd('C:\\Venkat\\Villanova\\Semester 4\\MSA 8220 - Analytical Methods for Data Mining\\Final Project\\Model_R')
set1 = read.csv('cali1.csv')
set2 = read.csv('cali2_SOLN.csv')
dataset = rbind(set1,set2)
#change variable names
colnames(dataset) = c('Lot_Size', 'Num_of_Bedrooms', 'Unit_Type',
'Electric_Cost','Internet_Access','Fiber_Optic',
'Heatfuel_Type','Num_of_Rooms','Water_Cost',
'Built_Year','Ownership_Status','Mortgage_Equity_Status',
'Fam_Type_Emp_Status','Household_Language',
'Fam_Type_Household_Status','Children_Age',
'Residence_Duration','Family_Size','Num_of_Children',
'Under_18','Over_60','Over_65','Num_of_Workers',
'Household_Work_Exp','Household_Work_Status',
'Num_of_Vehicles','Mobile_Broadband','High_Worth')
#data preparation and analysis
str(dataset)
#change the modeling type of variables
dataset$Lot_Size = factor(dataset$Lot_Size)
dataset$Num_of_Bedrooms = as.numeric(dataset$Num_of_Bedrooms)
dataset$Unit_Type = factor(dataset$Unit_Type)
dataset$Electric_Cost = as.numeric(dataset$Electric_Cost)
dataset$Internet_Access = factor(dataset$Internet_Access)
dataset$Fiber_Optic = factor(dataset$Fiber_Optic)
dataset$Heatfuel_Type = factor(dataset$Heatfuel_Type)
dataset$Num_of_Rooms = as.numeric(dataset$Num_of_Rooms)
dataset$Water_Cost = as.numeric(dataset$Water_Cost)
dataset$Built_Year = factor(dataset$Built_Year)
dataset$Ownership_Status = factor(dataset$Ownership_Status)
dataset$Mortgage_Equity_Status = factor(dataset$Mortgage_Equity_Status)
dataset$Fam_Type_Emp_Status = factor(dataset$Fam_Type_Emp_Status)
dataset$Household_Language = factor(dataset$Household_Language)
dataset$Fam_Type_Household_Status = factor(dataset$Fam_Type_Household_Status)
dataset$Children_Age = factor(dataset$Children_Age)
dataset$Residence_Duration = factor(dataset$Residence_Duration)
dataset$Family_Size = as.numeric(dataset$Family_Size)
dataset$Num_of_Children = as.numeric(dataset$Num_of_Children)
dataset$Under_18 = factor(dataset$Under_18)
dataset$Over_60 = factor(dataset$Over_60)
dataset$Over_65 = factor(dataset$Over_65)
dataset$Num_of_Workers = as.numeric(dataset$Num_of_Workers)
dataset$Household_Work_Exp = factor(dataset$Household_Work_Exp)
dataset$Household_Work_Status = factor(dataset$Household_Work_Status)
dataset$Num_of_Vehicles = as.numeric(dataset$Num_of_Vehicles)
dataset$Mobile_Broadband = factor(dataset$Mobile_Broadband)
dataset$High_Worth = as.vector(dataset$High_Worth)
dataset$High_Worth
class(dataset$High_Worth)
impute_dataset = mice(data = dataset,
m = 1,
maxit = 3)
imputed_dataset = complete(impute_dataset,1)
set.seed(5)
split = sample.split(Y = imputed_dataset$High_Worth,
SplitRatio = 0.75)
training_set = subset(imputed_dataset, split == TRUE)
test_set = subset(imputed_dataset, split == FALSE)
#build the decision tree classifier using training set
boostedtree_classifier = gbm(formula = High_Worth ~ .,
data = training_set,
distribution = "bernoulli",
n.trees = 100,
interaction.depth = 3)
#predict the test set
y_pred = predict(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 100)
y_pred = predict(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 100,
type = 'response')
#predict the test set
y_prob = predict(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 100,
type = 'response')
y_pred = ifelse(y_prod >= 0.5, 1, 0)
y_pred = ifelse(y_prob >= 0.5, 1, 0)
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
#validate the model using k-fold cross validation
folds = createFolds(y = imputed_dataset$High_Worth,
k = 10)
accuracy = lapply(folds, function(x){
training_fold = imputed_dataset[-x,]
test_fold = imputed_dataset[x,]
folds_boostedtree_classifier = gbm(formula = High_Worth ~ .,
data = training_fold,
distribution = "bernoulli",
n.trees = 100,
interaction.depth = 3)
folds_y_prob = predict(folds_boostedtree_classifier,
newdata = test_fold[,-28],
n.trees = 100,
type = 'response')
folds_y_pred = ifelse(folds_y_prob >= 0.5, 1, 0)
folds_cm = table(test_fold[,28], folds_y_pred)
folds_accuracy = (folds_cm[1,1] + folds_cm[2,2])/(folds_cm[1,1] + folds_cm[1,2] + folds_cm[2,1] + folds_cm[2,2])
return(folds_accuracy)
})
cm
y_pred
summary(y_pred)
cm = table(test_set[,28], y_pred)
cm
y_prob = predict(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 100,
type = 'response')
y_prob
y_prob = predict.gbm(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 100,
type = 'response')
y_prob
y_pred = ifelse(y_prob >= 0.191, 1, 0)
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.188, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
y_pred = ifelse(y_prob >= 0.195, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.2, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.25, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred
y_prob
y_pred = ifelse(y_prob >= 0.22, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.215, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.223, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
y_pred = ifelse(y_prob >= 0.211, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.21, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.205, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.215, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.22, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.221, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.222, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.2, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
boostedtree_classifier = gbm(formula = High_Worth ~ .,
data = training_set,
distribution = "bernoulli",
n.trees = 500,
interaction.depth = 5)
#predict the test set
y_prob = predict.gbm(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 100,
type = 'response')
y_pred = ifelse(y_prob >= 0.22, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.21, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.225, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.22, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
boostedtree_classifier = gbm(formula = High_Worth ~ .,
data = training_set,
distribution = "bernoulli",
n.trees = 500,
interaction.depth = 3)
#predict the test set
y_prob = predict.gbm(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 100,
type = 'response')
y_pred = ifelse(y_prob >= 0.22, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_prob = predict.gbm(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 500,
type = 'response')
y_prob
y_pred = ifelse(y_prob >= 0.22, 1, 0)
cm = table(test_set[,28], y_pred)
cm
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.23, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.24, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_prob = predict.gbm(boostedtree_classifier,
newdata = test_set[,-28],
n.trees = 500,
type = 'response')
y_pred = ifelse(y_prob >= 0.25, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.26, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.27, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.28, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.29, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.30, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.295, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.285, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
y_pred = ifelse(y_prob >= 0.291, 1, 0)
#build the confusion matrix
cm = table(test_set[,28], y_pred)
cm
#determine the misclassification rate
misclassification_rate = (cm[2,1] + cm[1,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
misclassification_rate
folds = createFolds(y = imputed_dataset$High_Worth,
k = 10)
accuracy = lapply(folds, function(x){
training_fold = imputed_dataset[-x,]
test_fold = imputed_dataset[x,]
folds_boostedtree_classifier = gbm(formula = High_Worth ~ .,
data = training_fold,
distribution = "bernoulli",
n.trees = 500,
interaction.depth = 3)
folds_y_prob = predict.gbm(folds_boostedtree_classifier,
newdata = test_fold[,-28],
n.trees = 500,
type = 'response')
folds_y_pred = ifelse(folds_y_prob >= 0.29, 1, 0)
folds_cm = table(test_fold[,28], folds_y_pred)
folds_accuracy = (folds_cm[1,1] + folds_cm[2,2])/(folds_cm[1,1] + folds_cm[1,2] + folds_cm[2,1] + folds_cm[2,2])
return(folds_accuracy)
})
mean_accuracy = mean(as.numeric(accuracy))
mean_accuracy
